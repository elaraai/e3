# E3: East Execution Engine

E3 is an execution engine for data projects. It manages reproducible computations across multiple runtimes (Node.js, Python, Julia), with content-addressed caching and reactive dataflow.

## Overview

E3 provides the infrastructure for "data science projects" - consulting engagements, ML pipelines, simulations, analytics - where you need to:

- **Iterate locally**: Write code, test with sample data, see results immediately
- **Deploy reliably**: Push the exact same computation to a server
- **Cache aggressively**: Never recompute what you've already computed
- **Mix runtimes**: Use scikit-learn from Python, simulations from Julia, glue logic in TypeScript
- **React to changes**: When input data updates, propagate changes through the pipeline

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│  Working Copy (your project directory)                      │
│  ├── inputs/           # Dataset files (.beast2, .east)     │
│  ├── outputs/          # Computed results                   │
│  └── e3.toml           # Project configuration              │
├─────────────────────────────────────────────────────────────┤
│  .e3/ (repository - like .git/)                             │
│  ├── objects/          # Content-addressed storage          │
│  ├── packages/         # Installed packages (inc. modules)  │
│  ├── runtimes/         # Runtimes (node, python, julia)     │
│  ├── tasks/            # Task definitions                   │
│  ├── executions/       # Task execution history/logs/etc    │
│  └── logs/             # Execution logs                     │
└─────────────────────────────────────────────────────────────┘
```

The `.e3/` directory is a cache and package store. Your working copy - configuration, datasets, task definitions - lives outside it. Like git, you can delete `.e3/` and reconstruct it from your working copy (though you'd lose cached execution results).

E3 is agnostic to how East modules are authored. You might write them in TypeScript using the fluent API, or eventually in a native East syntax. The compiled modules (IR) are what e3 consumes - typically installed via packages.

### Key Concepts

**Modules** are units of East code that export a value (a library typically exports a struct of functions, while a runnable program _is_ a module that exports its "main" function). Some modules are provided by runtimes instead of being defined by East IR. See `east.md` for the module system specification.

**Packages** bundle modules, runtimes, datasets, and install scripts for distribution. When you `e3 add east-python`, you get the Python runtime plus platform modules like `east-python/fs`.

**Runtimes** or "runners" are programs that can execute East programs as e3 tasks (e.g. our JavaScript interpretter or Julia compiler).

**Tasks** are a combination of a function module and the runtime that executes it, and defines programs that _can be_ run on e3.

**Executions** are tasks executions with specific inputs. A task's identity is the hash of its module plus its input hashes - same inputs always produce the same task ID, enabling memoization.

**Dataflows** are DAGs of tasks over files. Define which tasks read which files and produce which outputs, then `e3 go` executes the whole pipeline (like `make`), or `e3 watch` keeps outputs up-to-date reactively.

### Example Workflow

```bash
# Initialize a repository for production data
$ cd ~/data/client-abc
$ e3 init
Created .e3/ repository

# Install the Python runner from the public registry
$ e3 add east-python
Installing east-python... done

# Install your team's package from a local build
$ e3 add ~/dev/acme-forecast/dist/acme-forecast-1.2.0.pkg.beast2
Installing acme-forecast... done

# Or install from a private registry
$ e3 add acme-forecast --registry https://packages.acme.internal

# Run a task from your package against the local data
$ e3 run acme-forecast/train ./sales.beast2 -o ./model.beast2
Running acme-forecast/train... done (2.3s)

# Run again with same inputs - instant (cached)
$ e3 run acme-forecast/train ./sales.beast2 -o ./model.beast2
Cached (0.01s)

# Execute the full dataflow pipeline
$ e3 go
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for input changes and re-run affected tasks
$ e3 watch
Watching inputs/... (Ctrl+C to stop)
```

This pattern - installing a package into different repositories - supports common scenarios:
- **Testing**: Smoke test repo, integration test repo, production repo
- **Data governance**: Bring code to the data, not data to the code
- **Environment isolation**: Same package, different configurations

## Repository Structure

An e3 repository consists of a **working copy** (your project directory) and a **repository store** (`.e3/`).

### Working Copy

The working copy is a materialized view of the currently checked-out package's datasets:

```
~/data/client-abc/
├── e3.toml                        # Repo config (registries, settings)
├── inputs/
│   ├── sales.east                 # <10MB, human-readable
│   └── reference.beast2           # ≥10MB, binary
│   └── reference.partial.east     # Truncated preview of large file
└── outputs/
    ├── preprocess/
    │   └── cleaned.east
    ├── train/
    │   └── model.beast2
    │   └── model.partial.east
    └── predict/
        └── forecast.east
```

**Key points:**

- **Datasets only** - modules, tasks, and dataflows are defined in packages (via TypeScript), not as files in the working copy
- **Automatic format selection**:
  - Files <10MB → `.east` (human-readable text)
  - Files ≥10MB → `.beast2` (binary) + `.partial.east` (truncated preview)
- **Flexible inputs** - e3 accepts `.beast2`, `.east`, or `.json` input files
- **Outputs organized by dataflow step** - `outputs/<step>/<name>.<format>`

**`e3.toml`** - Minimal repo-level configuration:

```toml
[registries]
default = "https://packages.east-lang.org"
acme = "https://packages.acme.internal"

[settings]
default_runtime = "east-python"
```

### Package Checkout

The working copy shows whichever package is currently checked out:

```bash
# See current package
$ e3 status
Package: acme-forecast@0.21.1
Status: clean

# Checkout a different package (or version)
$ e3 checkout acme-forecast@0.20.0
Switched to acme-forecast@0.20.0

# Checkout your local development package
$ e3 checkout my-project
Switched to my-project@0.1.0
```

When you checkout a package version, e3 materializes that version's datasets into the working directory. Unchanged files are untouched (content-addressed).

### Repository Store (`.e3/`)

The `.e3/` directory contains all package data and execution history:

```
.e3/
├── config.toml              # Repo-level config (copy of e3.toml)
├── HEAD                     # Currently checked-out package@version
├── objects/                 # Content-addressed blob storage
│   ├── ab/
│   │   └── cd1234...beast2
│   └── ef/
│       └── 567890...beast2
├── packages/                # Installed packages (exploded)
│   ├── east-python/
│   │   └── 1.2.0.beast2     # Package manifest (refs to objects/)
│   └── acme-forecast/
│       ├── 0.20.0.beast2
│       └── 0.21.1.beast2
├── runtimes/                # Runtime installations
│   ├── east-node/
│   │   └── node_modules/
│   ├── east-python/
│   │   └── venv/
│   └── east-julia/
│       └── env/
├── executions/              # Execution cache
│   └── <execution_id>.beast2
└── logs/                    # Streaming execution logs
    └── <execution_id>.log
```

#### `objects/`

Content-addressed storage for all immutable data. Objects are stored by SHA256 hash, split by first two hex characters (like git). Everything ends up here: module IR, datasets, execution results.

#### `packages/`

Installed packages in exploded form. Each `<package>/<version>.beast2` is a manifest containing references to objects in `objects/`. See the Packages section for the full structure.

#### `runtimes/`

Runtime-specific installations managed by runtime packages. When you `e3 add east-python`:
1. Package manifest installed to `packages/east-python/`
2. Install script runs `uv` to create venv in `runtimes/east-python/`

Multiple packages can share a runtime.

#### `executions/`

Cached execution results indexed by execution ID (hash of task + input hashes). Enables memoization - same inputs always hit the cache.

#### `logs/`

Streaming log files written during execution. Allows `e3 logs --follow` to tail in real-time.

### Versioning

Packages have semver versions. The checked-out package can be committed to create new versions:

```bash
$ e3 status
Package: my-project@0.21.1
Modified:
  inputs/sales.east
  outputs/train/model.beast2

$ e3 commit --patch -m "Updated Q4 sales data"
Committed my-project@0.21.2

$ e3 checkout my-project@0.21.1
Switched to my-project@0.21.1
# Working directory now shows old version's datasets

$ e3 go
[1/2] train... cached    # Cache hit from original run!
[2/2] predict... cached
```

### Reconstruction

To reproduce a repo elsewhere:

1. Copy `e3.toml` (registries)
2. `e3 init && e3 add <package>` (reinstall packages)
3. `e3 checkout <package>@<version>` (get datasets)
4. `e3 go` (re-execute, will hit caches if same inputs)

## Packages

A **package** bundles everything needed to run computations: modules, tasks, dataflows, and datasets. Packages are:
- **Defined in TypeScript** using the e3 SDK
- **Distributed as `.pkg.beast2`** files (bundled form)
- **Installed into `.e3/`** in exploded form (refs to objects)

### Package Type

```
PackageType = Struct {
    name: String,
    version: String,
    dependencies: Dict<String, VersionConstraint>,

    modules: Dict<String, Module>,
    tasks: Dict<String, Task>,
    dataflows: Dict<String, Dataflow>,
    datasets: Dict<String, Dataset>,

    runtime: Option<RuntimeDef>,
    scripts: Option<Scripts>,
}

TaskType = Struct {
    module: String,     // module name or "pkg/module"
    runtime: String,    // runtime name
}

DataflowType = Struct {
    task: String,
    inputs: Array<String>,
    output: String,
}

DatasetType = Struct {
    path: String,       // relative path in working dir
    hash: String,       // content hash
    type: Option<EastType>,
}

RuntimeDefType = Struct {
    name: String,
    command: Array<CommandPart>,
}

ScriptsType = Struct {
    install: Option<String>,
    uninstall: Option<String>,
}
```

### Bundled vs Exploded

**Bundled** (`.pkg.beast2` for distribution):
- All content inline
- Self-contained, can be copied anywhere
- Produced by TypeScript build

**Exploded** (installed in `.e3/`):
- Content stored in `objects/`
- Package manifest contains hash references
- Dependencies resolved to specific version hashes

```bash
# Install from local build
$ e3 add ~/dev/acme-forecast/dist/acme-forecast-0.21.0.pkg.beast2

# Install from registry
$ e3 add acme-forecast@0.21.0 --registry https://packages.acme.internal
```

### Package Namespacing

Package names provide namespaces for their contents:

```bash
# Run a task from a package
$ e3 run acme-forecast/train inputs/sales.east

# Reference a module from another package
task = "acme-forecast/train"
```

### Creating Packages

Packages are defined in TypeScript using the e3 SDK:

```typescript
import { e3, East } from '@anthropic/e3-sdk';

const trainModule = East.module("train", ($) => {
    // ... East code
});

const trainTask = e3.task("train", {
    module: trainModule,
    runtime: "east-python",
});

const pipeline = e3.dataflow("run-training", {
    task: trainTask,
    inputs: ["inputs/sales"],
    output: "outputs/train/model",
});

export default e3.package({
    name: "acme-forecast",
    version: "0.21.0",
    dependencies: { "east-python": "^1.0.0" },
    modules: { train: trainModule },
    tasks: { train: trainTask },
    dataflows: { "run-training": pipeline },
    datasets: {
        "inputs/demo-sales": demoData,
    },
});
```

Build with: `npx e3-build` → produces `acme-forecast-0.21.0.pkg.beast2`

## Tasks & Execution

A **task** defines something that *can be* executed: a module bound to a runtime.

### Task Definition

```
TaskType = Struct {
    module: String,     // "train" (same package) or "other-pkg/train"
    runtime: String,    // "east-python", "east-node", "east-julia"
}
```

Tasks are defined in packages. The runtime determines which execution environment runs the East IR.

### Execution

An **execution** is a task invoked with specific inputs. Execution identity:

```
execution_id = hash(task_hash, input_hashes...)
```

Same task + same inputs = same execution ID = cache hit.

### Running Tasks

```bash
# Ad-hoc run (specify I/O explicitly)
$ e3 run acme-forecast/train inputs/sales.east -o outputs/model.beast2
Running acme-forecast/train... done (2.3s)

# Run again - cache hit
$ e3 run acme-forecast/train inputs/sales.east -o outputs/model.beast2
Cached (0.01s)

# Run from a different package
$ e3 run other-pkg/preprocess inputs/raw.json -o inputs/clean.east
```

### Execution Process

Each task execution runs as a **separate process**:

1. e3 resolves the task's module and runtime
2. e3 spawns the runtime process with the module IR and input paths
3. Runtime loads IR, deserializes inputs, executes, serializes output
4. e3 stores result in `executions/`, updates cache
5. Output written to specified path

Logs stream to `.e3/logs/<execution_id>.log` during execution.

### Memoization

Executions are cached by their ID (hash of task + inputs). When you run a task:

1. Compute execution ID from task hash + input content hashes
2. Check if `executions/<execution_id>.beast2` exists
3. If cached: return result immediately
4. If not: execute, cache result, return

This means:
- Changing inputs → new execution (cache miss)
- Changing module code → new task hash → new execution
- Re-running unchanged computation → instant cache hit

## Dataflows

A **dataflow** connects a task to file paths - it defines where inputs come from and where outputs go.

### Dataflow Definition

```
DataflowType = Struct {
    task: String,           // task name
    inputs: Array<String>,  // input dataset paths
    output: String,         // output dataset path (without step prefix)
}
```

Outputs are automatically organized under `outputs/<dataflow_name>/<output>`:

```typescript
e3.dataflow("train", {
    task: "train",
    inputs: ["inputs/sales"],
    output: "model",  // → outputs/train/model.east
});
```

### Running Dataflows

```bash
# Run all dataflows (like `make`)
$ e3 go
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for changes and re-run affected dataflows
$ e3 watch
Watching inputs/... (Ctrl+C to stop)
```

`e3 go` topologically sorts dataflows by their input/output dependencies and executes them in order. Cached results are used when inputs haven't changed.

### Dataflow DAG

The full DAG is implicit - all dataflows in the package, connected by their input/output paths:

```typescript
// These form a DAG: preprocess → train → predict
e3.dataflow("preprocess", {
    task: "preprocess",
    inputs: ["inputs/raw"],
    output: "cleaned",  // → outputs/preprocess/cleaned.east
});

e3.dataflow("train", {
    task: "train",
    inputs: ["outputs/preprocess/cleaned"],
    output: "model",  // → outputs/train/model.east
});

e3.dataflow("predict", {
    task: "predict",
    inputs: ["outputs/train/model", "inputs/new-data"],
    output: "forecast",  // → outputs/predict/forecast.east
});
```

### Selective Execution

```bash
# Run only dataflows matching a pattern
$ e3 go --filter "train*"

# Run a specific dataflow
$ e3 go train
```

## CLI Reference

### Repository Management

```bash
e3 init                      # Create new repo
e3 status                    # Show current package, modified files, pending dataflows
e3 add <package>             # Install package from registry or local file
e3 remove <package>          # Uninstall package
```

### Package Operations

```bash
e3 checkout <pkg>[@<ver>]    # Switch to package (and version)
e3 commit --patch|-minor|-major -m "msg"  # Commit new version
e3 log                       # Show version history
e3 diff [<path>]             # Show changes since last commit
e3 reset [<path>]            # Discard uncommitted changes
```

### Execution

```bash
e3 run <task> <inputs...> -o <output>  # Ad-hoc task execution
e3 go [--filter <pattern>]             # Run dataflows
e3 watch                               # Watch mode - re-run on changes
```

### Inspection

```bash
e3 logs [<execution_id>] [--follow]    # View execution logs
e3 view <path>                         # TUI data viewer
e3 convert <path> [--format east|json] # Convert between formats
```

### Registry

```bash
e3 publish                   # Publish current package to registry
e3 search <query>            # Search registry for packages
```

### Examples

```bash
# Full workflow
$ cd ~/data/client-project
$ e3 init
$ e3 add east-python
$ e3 add ~/dev/my-package/dist/my-package-1.0.0.pkg.beast2
$ e3 checkout my-package
$ e3 go
$ e3 view outputs/predict/forecast.east
$ e3 commit --patch -m "Initial run with production data"
$ e3 checkout my-package@1.0.0  # Time travel to this version
```
