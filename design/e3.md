# E3: East Execution Engine

E3 is an execution engine for data projects. It manages reproducible computations across multiple runtimes (Node.js, Python, Julia), with content-addressed caching and reactive dataflow.

## Overview

E3 provides the infrastructure for "data science projects" - consulting engagements, ML pipelines, simulations, analytics - where you need to:

- **Iterate locally**: Write code, test with sample data, see results immediately
- **Deploy reliably**: Push the exact same computation to a server
- **Cache aggressively**: Never recompute what you've already computed
- **Mix runtimes**: Use scikit-learn from Python, simulations from Julia, glue logic in TypeScript
- **React to changes**: When input data updates, propagate changes through the pipeline

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│  Working Copy (your project directory)                      │
│  ├── inputs/           # Dataset files (.beast2, .east)     │
│  ├── outputs/          # Computed results                   │
│  └── e3.east           # Project configuration              │
├─────────────────────────────────────────────────────────────┤
│  .e3/ (repository - like .git/)                             │
│  ├── objects/          # Content-addressed storage          │
│  ├── packages/         # Installed packages (inc. modules)  │
│  ├── runtimes/         # Runtimes (node, python, julia)     │
│  ├── tasks/            # Task definitions                   │
│  ├── executions/       # Task execution history/logs/etc    │
│  └── logs/             # Execution logs                     │
└─────────────────────────────────────────────────────────────┘
```

The `.e3/` directory is a cache, execution log and package store with some similarities to git's `.git/` directory.

Packages can serve different purposes, from support libraries to client project code, and at any point in time there is a "current" or "checked out" package. When a package is checked out, a human-readable "working copy" of the datasets associated with that package outside the `.e3/` directory for easy viewing. You can initiate the dataflow for the current checked out package with `e3 start`, which will launch tasks as necessary to produce all the outputs will be populated in `.e3/`. If you switch your current package and back again, the same data will be reproduced from cache. If you run the same package in a different e3 repository (even on a different computer), the same outputs will be reproduced.

E3 is agnostic to how East modules are authored. You might write them in TypeScript using the fluent API, or eventually in a native East syntax. The compiled modules (IR) are what e3 consumes - typically installed via packages.

### Key Concepts

**Modules** are units of East code that export a value (a library typically exports a struct of functions, while a runnable program _is_ a module that exports its "main" function). Some modules are provided by runtimes instead of being defined by East IR. See `east.md` for the module system specification.

**Packages** bundle modules, runtimes, datasets, and install scripts for distribution. When you `e3 add east-python`, you get the Python runtime plus platform modules like `east-python/fs`. There is exactly one current or checked-out package at any time.

**Runtimes** or "runners" are programs that can execute East programs as e3 tasks (e.g. our JavaScript interpretter or Julia compiler).

**Tasks** are a combination of a function module and the runtime that executes it, and defines programs that _can be_ run on e3.

**Executions** are tasks executions with specific inputs. A task's identity is the hash of its module plus its input hashes - same inputs always produce the same task ID, enabling memoization.

**Dataflows** are DAGs of tasks over files. Define which tasks read which files and produce which outputs, then `e3 start` executes the whole pipeline (like `make`), or `e3 start --watch` keeps outputs up-to-date reactively to changes to input files (maintaining internal consistency at all times).

The **working copy** is a human-readable copy of the dataflow inputs and outputs of the currently checked-out package. The filenames and directories are derived automatically from the name of the input datasets and dataflow steps.

### Example Workflow

```bash
# Initialize a repository for production data
$ cd ~/data/client-abc
$ e3 init
Created .e3/ repository

# Install the Python runner from the public registry
$ e3 add east-python
Installing east-python... done

# Install your team's package from a local build
$ e3 add ~/dev/acme-forecast/dist/acme-forecast-1.2.0.pkg.beast2
Installing acme-forecast... done

# Or install from a private registry
$ e3 registry add https://packages.acme.internal
$ e3 add acme-forecast

# Run a task from any package against data on your computer (.beast2, .east and .json inputs supported)
$ e3 run acme-forecast/train ./sales.beast2 -o ./model.beast2
Running acme-forecast/train... done (2.3s)

# Run again with same inputs - instant (cached)
$ e3 run acme-forecast/train ./sales.beast2 -o ./model.beast2
Cached (0.01s)

# Execute the full dataflow pipeline
$ e3 start
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for input changes and re-run affected tasks
$ e3 start --watch
Watching inputs/... (Ctrl+C to stop)
```

This pattern - installing a package into different repositories - supports common scenarios:
- **Testing**: Smoke test repo, integration test repo, production repo
- **Data governance**: Bring code to the data, not data to the code
- **Environment isolation**: Same package, different configurations

## Repository Structure

An e3 repository consists of a **working copy** (your project directory) and a **repository store** (`.e3/`).

### Working Copy

The working copy is a materialized view of the currently checked-out package's datasets:

```
~/data/client-abc/
├── e3.east                        # Repo config (registries, settings)
├── inputs/
│   ├── sales.east                 # <10MB, human-readable
│   └── reference.beast2           # ≥10MB, binary
│   └── reference.partial.east     # Truncated preview of large file
└── outputs/
    ├── preprocess/
    │   └── cleaned.east
    ├── train/
    │   └── model.beast2
    │   └── model.partial.east
    └── predict/
        └── forecast.east
```

**Key points:**

- **Datasets only** - modules, tasks, and dataflows are defined in packages (via TypeScript), not as files in the working copy
- **Automatic format selection**:
  - Files <10MB → `.east` (human-readable text)
  - Files ≥10MB → `.beast2` (binary) + `.partial.east` (truncated preview)
- **Flexible inputs** - e3 accepts `.beast2`, `.east`, or `.json` input files
- **Outputs organized by dataflow step** - `outputs/<step>/<name>.<format>`
- **Only a view** - you can fetch any dataset from any package with `e3 get <package>/outputs/<step>/<name>`

**`e3.east`** - Minimal repo-level configuration:

```
(
    registries = {
        "default": "https://packages.east-lang.org",
        "acme": "https://packages.acme.internal",
    },
    settings = {
        "default_runtime": "east-python",
    },
)
```

### Package Checkout

The working copy shows whichever package is currently checked out:

```bash
# See current package
$ e3 status
Package: acme-forecast@0.21.1
Status: clean

# Checkout a different package (or version)
$ e3 checkout acme-forecast@0.20.0
Switched to acme-forecast@0.20.0

# Checkout your local development package
$ e3 checkout my-project
Switched to my-project@0.1.0
```

When you checkout a package version, e3 materializes that version's datasets into the working directory. Unchanged files are untouched (content-addressed).

### Repository Store (`.e3/`)

The `.e3/` directory contains all package data and execution history:

```
.e3/
├── HEAD                     # Currently checked-out package@version
├── objects/                 # Content-addressed blob storage
│   ├── ab/
│   │   └── cd1234...
│   └── ef/
│       └── 567890...
├── packages/                # Installed packages (exploded)
│   ├── east-python/
│   │   └── 1.2.0            # Package manifest (refs to objects/)
│   └── acme-forecast/
│       ├── 0.20.0
│       └── 0.21.1
├── runtimes/                # Runtime installations
│   ├── east-node/
│   │   ├── command
│   │   └── node_modules/
│   ├── east-python/
│   │   └── venv/
│   └── east-julia/
│       └── env/
├── executions/              # Execution cache
│   └── <execution_id>.beast2
└── logs/                    # Streaming execution logs
    └── <execution_id>.log
```

#### `objects/`

Content-addressed storage for all immutable data. Objects are stored by SHA256 hash, split by first two hex characters (like git). Everything ends up here: module IR, datasets, execution results.

#### `packages/`

Installed packages in exploded form. Each `<package>/<version>.beast2` is a manifest containing references to objects in `objects/`. See the Packages section for the full structure.

#### `runtimes/`

Runtime-specific installations managed by runtime packages. When you `e3 add east-python`:
1. Package manifest installed to `packages/east-python/`
2. Install script runs `uv` to create venv in `runtimes/east-python/`
3. The command to launch the runner is created at `runtimes/east-python/command`

Multiple packages can share a runtime.

#### `executions/`

Cached execution results indexed by execution ID (hash of task + input hashes). Enables memoization - same inputs always hit the cache.

#### `logs/`

Streaming log files written during execution. Allows `e3 logs --follow` to tail in real-time.

### Versioning

Packages have semver versions. The checked-out package can be committed to create new versions:

```bash
$ e3 status
Package: my-project@0.21.1
Modified:
  inputs/sales.east
  outputs/train/model.beast2

$ e3 commit --patch -m "Updated Q4 sales data"
Committed my-project@0.21.2

$ e3 checkout my-project@0.21.1
Switched to my-project@0.21.1
# Working directory now shows old version's datasets

$ e3 start
[1/2] train... cached    # Cache hit from original run!
[2/2] predict... cached
```

Note the only changes that can be made to a package through the `e3` CLI tool are updating the dataset values.

### Extracting a package

Packages can be extracted from a single monolithic `.pkg.beast2` file, in the inverse process to package installation.

```bash
$ e3 bundle -o my-project.pkg.beast2

$ e3 bundle --project other-project -o other-project.pkg.beast2
```

The package bundle is ready to transfer to another computer and add to a remote repository.
This can be useful for bundling both code and the current value of the datasets (fully processed outputs, possibly with modified inputs).

### Reconstruction

To reproduce a repo elsewhere:

1. Copy `e3.east` (registries)
2. `e3 init && e3 add <package>` (reinstall packages)
3. `e3 checkout <package>@<version>` (set current package)
4. `e3 start` (re-execute, will hit caches if same inputs)

## Packages

A **package** bundles everything needed to run computations: modules, tasks, dataflows, and datasets. Packages are:
- **Defined in TypeScript** using the e3 SDK
- **Distributed as `.pkg.beast2`** files (bundled form)
- **Installed into `.e3/`** in exploded form (refs to objects)

### Package Type

```ts
type PackageType = StructType<{
    name: StringType,
    version: StringType, // possibly 3 integers?
    dependencies: DictType<StringType, VersionConstraintType>,

    modules: DictType<StringType, ModuleType>,
    tasks: DictType<StringType, TaskType>,
    dataflows: DictType<StringType, DataflowType>,
    datasets: DictType<StringType, DatasetType>,

    runtime: OptionType<RuntimeDefType>,
    scripts: OptionType<ScriptsType>,
}>;

type TaskType = StructType<{
    module: StringType,     // module name or "pkg/module"
    runtime: StringType,    // runtime name
}>;

type DataflowType = StructType<{
    task: StringType,
    inputs: ArrayType<StringType>,
    output: StringType,
}>;

type DatasetType = StructType<{
    path: StringType,             // relative path in working dir
    hash: OptionType<StringType>, // content hash (if value exists)
    type: EastType,
}>;

type RuntimeDefType = StructType<{
    name: StringType,
    command: ArrayType<CommandPart>,
}>;

type ScriptsType = StructType<{
    install: OptionType<StringType>,
    uninstall: OptionType<StringType>,
}>;
```

### Bundled vs Exploded

**Bundled** (`.pkg.beast2` for distribution):
- All content inline
- Self-contained, can be copied anywhere
- Produced by TypeScript build

**Exploded** (installed in `.e3/`):
- Content stored in `objects/`
- Package manifest contains hash references
- Dependencies resolved to specific version hashes

```bash
# Install from local build
$ e3 add ~/dev/acme-forecast/dist/acme-forecast-0.21.0.pkg.beast2

# Install from registry
$ e3 add acme-forecast@0.21.0 --registry https://packages.acme.internal
```

### Package Namespacing

Package names provide namespaces for their contents:

```bash
# Run a task from a package
$ e3 run acme-forecast/train inputs/sales.east

# Reference a module from another package
task = "acme-forecast/train"
```

### Creating Packages

Packages are defined in TypeScript using the e3 SDK:

```typescript
import { East } from '@elaraai/east';
import { e3 } from '@elaraai/e3-sdk';
import mlPkg from '@elaraai/python-ml';

// Write the code as an East module
const trainModule = East.module("train", ($) => {
    const ml = $.import(mlPkg.modules.ml);

    // ... East code
});

// input dataset - loaded from ./inputs/sales.* (user can provide .beast2, .east or .json)
const sales = e3.input("sales", ArrayType(...), /* default value goes here */)

// Define a dataflow step from a module
const pipeline = e3.dataflow("train", [sales], trainModule); 

// Or define it inline
const pipeline = e3.dataflow("train", [sales], $ => { ... }); 

// Construct the package bundle
const pkg = e3.package(
    {
        name: "acme-forecast",
        version: "0.21.0",
    },
    pipeline, // automatically infers any dependencies (input datasets, upstream dataflows, modules, package dependencies, etc)
    // ... can add more
);

await pkg.save(/* defaults to save at ./acme-forecast-0.21.0.pkg.beast2 */);

export default pkg; // ready to import by other packages
```

There is a lot of freedom to import and bundle "pure" East modules defined in other npm packages, dynamically link with e3 packages, define logic inline, or more.

Simply run the script to produce the `acme-forecast-0.21.0.pkg.beast2` bundle.

## Tasks & Execution

A **task** defines something that *can be* executed: a module bound to a runtime.

### Task Definition

```
TaskType = Struct {
    module: String,     // "train" (same package) or "other-pkg/train"
    runtime: String,    // "east-python", "east-node", "east-julia"
}
```

Tasks are defined in packages. The runtime determines which execution environment runs the East IR. Generally this is inferred and created from the dependencies, but a task runtime can be explicitly created.

### Execution

An **execution** is a task invoked with specific inputs. Execution identity:

```
execution_id = hash(task_hash, input_hashes...)
```

Same task + same inputs = same execution ID = cache hit.

### Running Tasks

```bash
# Ad-hoc run (specify I/O explicitly)
$ e3 run acme-forecast/train inputs/sales.east -o outputs/model.beast2
Running acme-forecast/train... done (2.3s)

# Run again - cache hit
$ e3 run acme-forecast/train inputs/sales.east -o outputs/model.beast2
Cached (0.01s)

# Run from a different package
$ e3 run other-pkg/preprocess inputs/raw.json -o inputs/clean.east
```

### Execution Process

Each task execution runs as a **separate process**:

1. e3 resolves the task's module and runtime
2. e3 spawns the runtime process with the module IR and input paths, pipes logs
3. Runtime loads IR, deserializes inputs, executes, serializes output
4. e3 stores result in `executions/`, updates cache
5. Output written to specified path

Logs stream to `.e3/logs/<execution_id>.log` during execution.

### Memoization

Executions are cached by their ID (hash of task + inputs). When you run a task:

1. Compute execution ID from task hash + input content hashes
2. Check if `executions/<execution_id>.beast2` exists
3. If cached: return result immediately
4. If not: execute, cache result, return

This means:
- Changing inputs → new execution (cache miss)
- Changing module code → new task hash → new execution
- Re-running unchanged computation → instant cache hit

## Dataflows

A **dataflow** connects a task to file paths - it defines where inputs come from and where outputs go.

### Dataflow Definition

```ts
type DataflowType = StructType<{
    name: StringType,               // dataflow name
    task: StringType,               // task name
    inputs: ArrayType<StringType>,  // input dataset paths
    output: StringType,             // output dataset name (currently "output")
}>;
```

Outputs are automatically organized under `outputs/<dataflow_name>/output`:

```typescript
e3.dataflow("train", [sales], trainTask);
```

In future, we'll add the ability to destructure outputs to allow multiple named outputs.
We can similarly generalize inputs to enable different patterns, including scatter-gather tasks, etc.

### Running Dataflows

```bash
# Run all dataflows (like `make`)
$ e3 start
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for changes and re-run affected dataflows
$ e3 start --watch
Watching inputs/... (Ctrl+C to stop)
```

`e3 start` topologically sorts dataflows by their input/output dependencies and executes them in order. Cached results are used when inputs haven't changed.
With the `--watch` flag it will using inotify to watch for changed values and propagate them.

### Dataflow DAG

The full DAG is implicit - all dataflows in the package, connected by their input/output paths:

```typescript
// These form a DAG: preprocess → train → predict
e3.dataflow("preprocess", {
    task: "preprocess",
    inputs: ["inputs/raw"],
    output: "cleaned",  // → outputs/preprocess/cleaned.east
});

e3.dataflow("train", {
    task: "train",
    inputs: ["outputs/preprocess/cleaned"],
    output: "model",  // → outputs/train/model.east
});

e3.dataflow("predict", {
    task: "predict",
    inputs: ["outputs/train/model", "inputs/new-data"],
    output: "forecast",  // → outputs/predict/forecast.east
});
```

### Selective Execution

```bash
# Run only dataflows matching a pattern
$ e3 start --filter "train*"

# Run a specific dataflow
$ e3 start train
```

## CLI Reference

### Repository Management

```bash
e3 init                      # Create new repo
e3 status                    # Show current package, modified files, pending dataflows
e3 add <package>             # Install package from registry or local file
e3 remove <package>          # Uninstall package
```

### Package Operations

```bash
e3 checkout <pkg>[@<ver>]    # Switch to package (and version)
e3 commit --patch|-minor|-major -m "msg"  # Commit new version
e3 log                       # Show version history
e3 diff [<path>]             # Show changes since last commit
e3 reset [<path>]            # Discard uncommitted changes
```

### Execution

```bash
e3 run <task> <inputs...> -o <output>  # Ad-hoc task execution
e3 start [--filter <pattern>]          # Run dataflows
e3 start --watch                       # Watch mode - re-run on changes
```

### Inspection

```bash
e3 logs [<execution_id>] [--follow]    # View execution logs
e3 view <path>                         # TUI data viewer
e3 convert <path> [--format east|json] # Convert between formats
```

### Registry

```bash
e3 publish                   # Publish current package to registry
e3 search <query>            # Search registry for packages
```

### Examples

```bash
# Full workflow
$ cd ~/data/client-project
$ e3 init
$ e3 add east-python
$ e3 add ~/dev/my-package/dist/my-package-1.0.0.pkg.beast2
$ e3 checkout my-package
$ e3 start
$ e3 view outputs/predict/forecast.east
$ e3 commit --patch -m "Initial run with production data"
$ e3 checkout my-package@1.0.0  # Time travel to this version
```
