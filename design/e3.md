# e3: East Execution Engine

e3 is an execution engine for data projects. It manages reproducible computations across multiple runtimes (Node.js, Python, Julia), with content-addressed caching and reactive dataflow.

## Overview

e3 provides the infrastructure for "data science projects" - consulting engagements, ML pipelines, simulations, analytics - where you need to:

- **Iterate locally**: Write code, test with sample data, see results immediately
- **Deploy reliably**: Push the exact same computation to a server
- **Cache aggressively**: Never recompute what you've already computed
- **Mix runtimes**: Use scikit-learn from Python, simulations from Julia, glue logic in TypeScript
- **React to changes**: When input data updates, propagate changes through the pipeline

### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│  . (your e3 repository directory)                               │
│  ├── objects/          # Content-addressed storage              │
│  ├── packages/         # Installed packages (refs)              │
│  ├── executions/       # Execution state and results            │
│  └── workspaces/       # Stateful dataset namespaces            │
└─────────────────────────────────────────────────────────────────┘
```

The e3 repository contains and runs your workspaces, while also acting as a cache, execution log and package store.

Workspaces are where you work with data and follow the "template" defined in the package deployed to that workspace. You can type `e3 start <workspace>` to execute the dataflow to produce results.

Packages can serve different purposes, from support libraries to client project code. When a package is deployed to a workspace, so are all of its dependencies (meaning each workspace is associated with a given "root" or "main" package). Packages are immutable, and if you run the same package in a different workspace or e3 repository (even on a different computer), the same outputs will be reproduced.

e3 is agnostic to how code and packages are authored. You might create packages in TypeScript using the fluent API, extract them from another repository, or eventually code them in a native East syntax. East tasks execute compiled modules (IR), installed via packages.

### Key Concepts

**Workspaces** are namespaces of interactive datasets that you can read and write. Each workspace is initialized by deploying a package to the workspace, and contains the datasets and dataflows (tasks producing datasets).

**Packages** are an immutable bundle of e3 objects - East modules, tasks, datasets, dataflows, and install scripts for distribution. When you `e3 package add . east-python`, you get the Python runtime plus platform modules like `east-python/fs`. Packages can depend on other packages.

**Modules** are units of East code that export a value (a library typically exports a struct of functions, while a runnable program _is_ a module that exports its "main" function). Note that some modules are provided directly by East runtimes instead of being defined by East IR. See `east.md` for the module system specification.

**Runtimes** or "runners" are programs that can execute East programs as e3 tasks (e.g. our JavaScript interpretter or Julia compiler).

**Tasks** are a combination of a function module and the runtime that executes it, and defines programs that _can be_ run on e3.

**Executions** are tasks executions with specific inputs. A task's identity is the hash of its module plus its input hashes - same inputs always produce the same task ID, enabling memoization.

**Dataflows** are DAGs of tasks over files. Define which tasks read which files and produce which outputs, then `e3 start` executes the whole pipeline (like `make`).

### Example Workflow

```bash
# Initialize a repository for production data
$ cd ~/data/client-abc
$ e3 init .
Created e3 repository

# Install the Python runner from the public registry
$ e3 package add . east-python
Installing east-python... done

# Install your team's package from a local .zip
$ e3 package import . ~/dev/acme-forecast/dist/acme-forecast-1.2.0.zip
Installing acme-forecast@1.2.0... done

# Or add from a private registry
$ e3 registry add . https://packages.acme.internal
$ e3 package add . acme-forecast
Installing acme-forecast@0.21.1... done

# Run a task ad-hoc against data on your computer
$ e3 run . acme-forecast/train ./sales.beast2 -o ./model.beast2
Running acme-forecast/train... done (2.3s)

# Run again with same inputs - instant (cached)
$ e3 run . acme-forecast/train ./sales.beast2 -o ./model.beast2
Cached (0.01s)

# Create a workspace and deploy a package
$ e3 workspace create . production
Created production workspace

$ e3 workspace deploy . production acme-forecast@0.21.1
Deploying acme-forecast@0.21.1 to production... done

# Execute the full dataflow pipeline
$ e3 start . production
[1/3] preprocess... done (0.5s)
[2/3] train... done (38.1s)
[3/3] predict... done (1.2s)

# Get/set datasets
$ e3 dataset get . production outputs/predict
$ e3 dataset set . production inputs/sales ./new_sales.beast2

# Rerun - only affected dataflows execute
$ e3 start . production
[1/3] preprocess... cached
[2/3] train... cached
[3/3] predict... done (1.1s)

# Export workspace as a package (includes deployed package + current data)
$ e3 workspace export . production ./handoff.zip
Created acme-forecast-0.21.1-a3f8b2c1.zip

# Colleague imports it
$ e3 package import . ./handoff.zip
Installing acme-forecast@0.21.1-a3f8b2c1... done

$ e3 workspace deploy . analysis acme-forecast@0.21.1-a3f8b2c1
# Now they have your exact data state
```

## Repository Structure

An e3 repository is a directory containing content-addressed storage and refs. Most files outside `objects/` are **refs** - small text files containing a SHA256 hash pointing to an object.

### Example Repository

```
~/data/client-abc/                    # Your e3 repository
├── objects/                          # Content-addressed storage (all data lives here)
│   ├── 3a/
│   │   └── 8f2b...                   # A package object
│   ├── 7c/
│   │   └── 91d4...                   # A module IR blob
│   ├── a1/
│   │   └── bc56...                   # A dataset value
│   └── f2/
│       └── e847...                   # An execution result
│
├── packages/                         # Installed packages (refs)
│   ├── east-python/
│   │   └── 1.2.0                     # Contains: 3a8f2b... → objects/3a/8f2b...
│   └── acme-forecast/
│       ├── 0.20.0                    # Contains: 7d3e1a...
│       └── 0.21.1                    # Contains: 9b4c2f...
│
├── executions/                       # Execution history for tasks
│   └── <task_hash>/                  # Task hash (hash of task object)
│       └── <input_hash>/             # Input hash (hash of all input values)
│           ├── stdout.txt            # Captured stdout (streamed during run)
│           ├── stderr.txt            # Captured stderr (streamed during run)
│           └── output                # Contains: f2e847... → objects/f2/e847...
│
└── workspaces/                       # Stateful dataset namespaces
    └── production.beast2             # Workspace state (deployment + data root)
```

### Directory Reference

#### `objects/`

Content-addressed storage for all immutable data. Objects are stored by SHA256 hash, split into subdirectories by first two hex characters (like git). Everything ends up here: module IR, task definitions, dataset values, execution results, package manifests.

#### `packages/`

Installed packages. Each `<name>/<version>` file is a ref pointing to a package object in `objects/`. Package objects contain refs to their modules, tasks, dataflows, and datasets.

When you `e3 package add . acme-forecast`:
1. Package object and all referenced objects stored in `objects/`
2. Ref created at `packages/acme-forecast/0.21.1`

#### `tasks/`

Materialized task environments and execution history. Organized by task hash (the SHA256 of the task object, which includes the module and exec command).

**`tasks/<hash>/bin/`** - Runtime environment created on first execution:
- For Python tasks: `uv sync` creates a venv here
- For Node tasks: `npm install` creates node_modules here
- For Julia tasks: `julia --project=. instantiate` here
- `run.sh` is the generated entry point that invokes the task

**`tasks/<hash>/executions/<input_hash>/`** - State for each execution:
- `stdout.txt`, `stderr.txt` - Captured output (streamed live during execution)
- `output` - Ref to the result object

Execution identity = hash(task_hash, input_hashes...). Same inputs → same execution directory → cache hit.

#### `workspaces/`

Stateful namespaces where you work with data. Each workspace has:

- `package` - Ref to the deployed package (e.g., `acme-forecast/0.21.1`)
- `root` - Ref to the root DataObject (a struct matching package's dataset schema)

Workspaces follow the "template" defined by their package: the dataflows determine what tasks run and how datasets connect. But the actual data values can differ between workspaces.

The workspace root is a tree structure (like a git commit's root tree) enabling:
- **Atomic updates**: Swap one ref to update entire workspace
- **Structural sharing**: Unchanged subtrees keep the same hash
- **Fast cloning**: Duplicate a workspace by copying two refs

### Refs Pattern

Most files outside `objects/` are refs - text files containing a SHA256 hash:

```bash
$ cat packages/acme-forecast/0.21.1
3a8f2b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a

$ cat workspaces/production/root
d4e5f6789abc0123456789abcdef0123456789abcdef0123456789abcdef0123
```

This indirection enables:
- **Deduplication**: Same data referenced from multiple places
- **Atomic updates**: Write new object, then update ref
- **Easy diffing**: Compare ref values to detect changes

The exceptions are:
- `executions/<hash>/<hash>/*.txt` - Log files streamed during execution
- `workspaces/<name>.beast2` - Binary state file (not a simple ref)

### Object Types

All data in `objects/` is one of these types:

| Object | Description | Defined in |
|--------|-------------|------------|
| **Package** | Bundle of modules, tasks, dataflows, datasets | Packages section |
| **Module** | East IR + imports | Packages section |
| **Task** | How to run a computation (init + run commands) | Tasks & Execution section |
| **Data** | East values as trees or blobs | Data Objects section |

Objects reference each other by hash. A package contains hashes pointing to its modules and tasks. A task contains hashes pointing to module IR and init files.

### Data Tree Objects

Data tree objects store East values as persistent trees with structural sharing (like git trees). Each tree object is a `.beast2` file containing an East variant:

```ts
type DataRefType = VariantType<{
    unassigned: NullType, // leaf for unassigned value (e.g. result of a pending task)
    null: NullType,       // leaf for inline null value (optimization for NullType)
    object: StringType,   // leaf for value in object store (hash of .beast2 object)
    tree: StringType,     // tree of data refs (hash of .beast2 object)
}>

type TreeObjectType<T extends EastType> =
  T extends ArrayType<infer U> ? ArrayType<DataRefType> :
  T extends DictType<infer K, infer V> ? Dict<K, DataRefType> :
  T extends StructType<infer Fields> ? StructType<{ [K in keyof Fields]: DataRefType }> :
  T extends VariantType<infer Cases> ? StructType<{ case: StringType /* keyof Cases */, value: DataRefType }>;
```

Ony compound values (array/dict/struct/variant) can be represented by tree objects - primitives must be object values (or null).
The set type is omitted because dict keys are stored directly in the tree object.
Doing the same for `Set<K>` (treating it similarly to `Dict<K, NullType>`) means the set tree object would contain the entire set value (and therefore offers no new capability).

**Example: Workspace root (struct)**

```east
(
    inputs = .tree "a1bc56...",
    outputs = .tree "d4e5f6...",
)
```

**Example: inputs struct**

```east
(
    sales = .tree "7c91d4...",
    features = .object "8d2e7f...",
)
```

**Example: sales array (large, split into tree)**

```east
[
    .object "f1a2b3...",    // chunk 0
    .object "c4d5e6...",    // chunk 1
    .object "g7h8i9...",    // chunk 2
]
```

**Example: outputs struct**

```east
(
    model = .unassigned,
    predictions = .unassigned,
)
```

**Type inference:** Data references don't include East types. The type is inferred by traversing the package's dataset schema in parallel with the object tree. The variant tag (blob/struct/array/dict/variant) tells the decoder how to interpret the object structure; the schema tells it what the values mean. Note each .beast2 object has a self-describing header, though. To minimize the .beast2 type header overhead in tree objects, the only "inline" value case is for `NullType` so that `DataRef` doesn't need a type parameter. This means the tree object types may include struct field names, variant case names, or dictionary key types (and East's key types are constrained and likely to be primitives or structs of a few primitives).

**Structural sharing:** When updating a single field, only the path from root to that field needs new objects. Unchanged subtrees keep their hashes, enabling fast atomic updates and cheap workspace cloning.

### Garbage Collection

`e3 gc` removes unreferenced objects. The object graph is traced from roots:

**Roots:**
- `packages/<name>/<version>` → PackageObject hash
- `workspaces/<ws>/package` → `<name>/<version>` (resolved via packages/)
- `workspaces/<ws>/root` → DataObject hash (workspace data tree)
- `tasks/<hash>/executions/<input_hash>/output` → DataObject hash

**Object references:**

| Object | References |
|--------|------------|
| PackageObject | `modules` → ModuleObject hashes |
| | `tasks` → TaskObject hashes |
| | `datasets.value` → TreeObject hash (root of data tree) |
| | `dependencies` → version strings (resolve via packages/) |
| TaskObject | `init_tree` → DataObject hashes (package.json, etc.) |
| | `run[].object` → ModuleObject hashes (must include ALL modules) |
| ModuleObject | None (imports resolved at task level) |
| TreeObject | `.object` refs → DataObject hashes (blobs) |
| | `.tree` refs → TreeObject hashes (subtrees) |
| | `.null` / `.unassigned` → None (terminal) |

**Important:** The TaskObject's `run` command must include `--module` entries for the main module AND all its transitive imports. The SDK computes this closure when building the task.

### Bundled vs Installed

**Bundled** (`.zip` for distribution):
```
acme-forecast-0.21.1.zip
├── manifest.east     # Package name, version, root object hash
└── objects/
    ├── 3a/8f2b...
    ├── 7c/91d4...
    └── ...
```
- All objects inline in a zip file
- Self-contained, can be copied anywhere
- Produced by TypeScript build, `e3 package export`, or `e3 workspace export`
- Streaming I/O via `yauzl`/`yazl` (no need to load into RAM)

**Installed** (in repository):
- Objects extracted to `objects/`
- Ref created at `packages/<name>/<version>`
- Deduped with other packages
- Ready to deploy to workspaces

## Packages

A **package** bundles everything needed to run computations: modules, tasks, dataflows, and datasets. Packages are:
- **Defined in TypeScript** using the e3 SDK
- **Distributed as `.zip`** files (bundled form)
- **Installed as refs** to objects in the repository

### Package Object

A package object is stored in `objects/` and contains refs (hashes) to other objects:

```ts
type PackageObject = StructType<{
    name: StringType,
    version: StringType,

    // Refs to other objects (hash strings)
    modules: DictType<StringType, StringType>,    // name → module object hash
    tasks: DictType<StringType, StringType>,      // name → task object hash

    // Dataset structure and values
    datasets: StructType<{
        schema: DatasetSchema,   // Defines tree vs blob structure (root is always .tree .struct)
        value: StringType,       // Hash of root TreeObject
    }>,
    dataflows: ArrayType<DataflowDef>,            // Orchestration rules

    // Package dependencies (resolved to specific versions)
    dependencies: DictType<StringType, StringType>,  // pkg name → version
}>;

// Dataset schema defines the workspace tree structure (what's a tree vs blob)
type DatasetSchema = VariantType<{
    blob: EastTypeValue,              // Leaf: task-managed, opaque to orchestrator
    tree: TreeSchema,                 // Branch: orchestrator-managed, supports iteration
}>;

type TreeSchema = VariantType<{
    struct: DictType<StringType, DatasetSchema>,  // Fixed fields
    array: DatasetSchema,                          // Dynamic indices, homogeneous elements
    dict: StructType<{                             // Dynamic keys
        key: EastTypeValue,
        value: DatasetSchema,
    }>,
    // Future variants
    // - variant cases
    // - recursive trees
}>;

// TreePath identifies a location in the dataset tree
type TreePath = ArrayType<PathComponent>;

type PathComponent = VariantType<{
    field: StringType,  // .field "name" - struct tree field
    glob: NullType,     // .glob - iterate over array/dict tree entries
    // Future variants:
    //  - variant cases (conditional execution)
    //  - access multiple struct fields
    //  - array index / dict key selection
    //  - recursive trees + loopy tasks
}>;

// Dataflow defines how orchestrator marshals inputs/outputs
type DataflowDef = VariantType<{
    task: StructType<{
        task: StringType,             // Task name in this package
        inputs: ArrayType<TreePath>,  // TreePath for each input
        output: TreePath,             // TreePath of output
    }>,
    shuffle: StructType<{
        input: TreePath,              // Source path with globs
        output: TreePath,             // Destination path with reordered globs
        order: ArrayType<IntegerType> // Permutation of the glob indices to apply
    }>,
    // Future variants:
    // - source: External data source (polling, webhooks)
    // - sink: External data sink (APIs, databases)
    // - cron: Time-triggered execution
}>;
```

**Example package object:**

```east
(
    name = "acme-forecast",
    version = "0.21.1",
    modules = {
        "train": "7c91d4...",
        "predict": "a3f8b2...",
    },
    tasks = {
        "train": "5e7a3b...",
        "predict": "c4d5e6...",
    },

    // Dataset structure and initial values
    datasets = (
        schema = .tree .struct {
            "inputs": .tree .struct {
                "sales": .tree .array .blob SalesRecordType,
                "features": .blob FeaturesType,
            },
            "outputs": .tree .struct {
                "model": .blob ModelType,
                "predictions": .tree .array .blob PredictionType,
            },
        },
        value = "f4a7c2...",  // Hash of root TreeObject (initial/current state)
    ),

    // Dataflows: how tasks are orchestrated
    dataflows = [
        .task (
            task = "train",
            inputs = [
                [.field "inputs", .field "sales", .glob],
                [.field "inputs", .field "features"],
            ],
            output = [.field "outputs", .field "model"],
        ),
        .task (
            task = "predict",
            inputs = [
                [.field "outputs", .field "model"],
                [.field "inputs", .field "sales", .glob],
            ],
            output = [.field "outputs", .field "predictions", .glob],
        ),
    ],

    dependencies = {
        "east-python": "1.2.0",
    },
)
```

### Module Object

Modules are stored as objects and passed to the runner via `--module` flags. Each module self-registers by name.

```ts
type ModuleObject = StructType<{
    name: StringType,                    // e.g. "acme-forecast/train"
    type: EastTypeValueType,             // exported type
    ir: IRType,                          // compiled East IR
    imports: SetType<StringType>,        // names of modules this imports
}>;
```

The module doesn't track where imports come from - that's resolved at the task level. The task object lists all `--module` paths, and the runner registers them by name before execution.

See **Tasks & Execution** for the task object structure, and **east.md** for the full module system specification.

### Bundled vs Installed

**Bundled** (`.zip` for distribution):
- All content inline in a zip file
- Self-contained, can be copied anywhere
- Produced by TypeScript build or `e3 workspace export`

**Installed** (in repository):
- Content stored in `objects/`
- Ref at `packages/<name>/<version>` points to package object
- Dependencies resolved to specific versions

```bash
# Import from local .zip
$ e3 package import . ~/dev/acme-forecast/dist/acme-forecast-0.21.0.zip

# Add from registry (fetches + imports)
$ e3 package add . acme-forecast@0.21.0

# Export an installed package to .zip
$ e3 package export . acme-forecast@0.21.0 ./acme-forecast-0.21.0.zip
```

### Package Namespacing

Package names provide namespaces for their contents:

```bash
# Run a task from a package
$ e3 run . acme-forecast/train inputs/sales.east -o model.beast2

# Reference a module from another package (in East code)
const ml = $.import("east-python/ml");
```

### Creating Packages

Packages are defined in TypeScript using the e3 SDK:

```typescript
import { East } from '@elaraai/east';
import { e3 } from '@elaraai/e3-sdk';
import mlPkg from '@elaraai/python-ml';

// Write the code as an East module
const trainModule = East.module("train", ($) => {
    const ml = $.import(mlPkg.modules.ml);

    // ... East code
});

// input dataset - loaded from ./inputs/sales.* (user can provide .beast2, .east or .json)
const sales = e3.input("sales", ArrayType(...), /* default value goes here */)

// Define a dataflow step from a module
const pipeline = e3.dataflow("train", [sales], trainModule); 

// Or define it inline
const pipeline = e3.dataflow("train", [sales], $ => { ... }); 

// Construct the package bundle
const pkg = e3.package(
    {
        name: "acme-forecast",
        version: "0.21.0",
    },
    pipeline, // automatically infers any dependencies (input datasets, upstream dataflows, modules, package dependencies, etc)
    // ... can add more
);

await pkg.save(/* defaults to save at ./acme-forecast-0.21.0.zip */);

export default pkg; // ready to import by other packages
```

There is a lot of freedom to import and bundle "pure" East modules defined in other npm packages, dynamically link with e3 packages, define logic inline, or more.

Simply run the script to produce the `acme-forecast-0.21.0.zip` bundle.

## Tasks & Execution

A **task** is an object that defines how to run a computation. Tasks are stored in `objects/` and referenced by packages.

### Task Object

```ts
type TaskObject = StructType<{
    // Files to materialize in bin/ before init runs
    init_tree: DictType<StringType, StringType>,  // path → object hash

    // Run once after tree is materialized (optional)
    init: OptionType<ArrayType<CommandPartType>>,

    // Run for each execution
    run: ArrayType<CommandPartType>,
}>;

type CommandPart = VariantType<{
    literal: StringType, // Literal string: "npm", "run", "--prefix"
    object: StringType,  // Path to object: ./objects/<hash>
    bin: NullType,       // Path to this task's bin/ directory
    input: IntegerType,  // Path to input N
    output: NullType,    // Path where output should be written
}>;
```

### Example: Node.js Task

```east
(
    init_tree = {
        "package.json": "abc123...",
        "package-lock.json": "def456...",
    },
    init = .some [
        .literal "npm",
        .literal "install",
        .literal "--prefix",
        .bin,
    ],
    run = [
        .bin, .literal "/node_modules/.bin/east",
        .literal "run",
        .literal "--runtime", .literal "@elaraai/east-node-fs",
        .literal "--module", .object "7c91d4...",
        .literal "--main", .literal "acme-forecast/train",
        .literal "--input", .input 0,
        .literal "--output", .output,
    ],
)
```

The `package.json` object contains:

```json
{
    "dependencies": {
        "@elaraai/East": "1.0.0",
        "@elaraai/east-node-fs": "1.2.0"
    }
}
```

### The East CLI

The `east` CLI (from `@elaraai/East`) runs East modules:

```bash
east run \
  --runtime @elaraai/east-node-fs \
  --module ./objects/7c91d4... \
  --main acme-forecast/train \
  --input ./inputs/sales.beast2 \
  --output ./outputs/model.beast2
```

- `--runtime <pkg>` - npm packages providing runtime modules (self-register their module names)
- `--module <path>` - paths to East IR module objects (self-register their names)
- `--main <name>` - which module to execute
- `--input <path>` - input data files
- `--output <path>` - where to write result

Runtime module packages export a record keyed by module name:

```typescript
// @elaraai/east-node-fs
export default {
    "east-node/fs": { 
        readFile: (path: EastString) => /* ... */,
        writeFile: (path: EastString, data: EastBytes) => /* ... */,
    },
};
```

### Execution

An **execution** is a task invoked with specific inputs. Execution identity:

```
input_hash = hash(input_hashes...)
```

Same task + same inputs = same execution directory = cache hit.

### Running Tasks

```bash
# Ad-hoc run (specify I/O explicitly)
$ e3 run . acme-forecast/train inputs/sales.east -o outputs/model.beast2
Running acme-forecast/train... done (2.3s)

# Run again - cache hit
$ e3 run . acme-forecast/train inputs/sales.east -o outputs/model.beast2
Cached (0.01s)

# Run from a different package
$ e3 run . other-pkg/preprocess inputs/raw.json -o inputs/clean.east
```

### Execution Process

Each task execution runs as a **separate process**:

1. e3 looks up the task object from `objects/`
2. If `tasks/<hash>/bin/` doesn't exist:
   - Materialize `init_tree` files into `bin/`
   - Run the `init` command (if present)
3. Run the `run` command, substituting:
   - `.bin` → `tasks/<hash>/bin/`
   - `.object <hash>` → `objects/<hash>`
   - `.input N` → path to input N
   - `.output` → output path
4. Store result in `objects/`, write ref to `tasks/<hash>/executions/<input_hash>/output`
5. Stdout/stderr streamed to `tasks/<hash>/executions/<input_hash>/*.txt`

### Memoization

Executions are cached by their ID (hash of task + inputs). When you run a task:

1. Compute input hash from input content hashes
2. Check if `tasks/<task_hash>/executions/<input_hash>/output` exists
3. If cached: return result immediately (read ref, fetch from objects/)
4. If not: execute, store result, write ref

This means:
- Changing inputs → new execution (cache miss)
- Changing module code → new task hash → new execution
- Re-running unchanged computation → instant cache hit

## Dataflows

A **dataflow** connects a task to dataset paths. Dataflows are defined inline in the package object (not separate objects).

```east
dataflows = {
    "train": (task = "train", inputs = ["inputs/sales"], output = "model"),
    "predict": (task = "predict", inputs = ["outputs/train/model"], output = "forecast"),
}
```

- `task` - name of a task in this package
- `inputs` - dataset paths to read
- `output` - name of output dataset (stored at `outputs/<dataflow>/output`)

In TypeScript:

```typescript
e3.dataflow("train", [sales], trainTask);
```

In future, we'll add the ability to destructure outputs to allow multiple named outputs.
We can similarly generalize inputs to enable different patterns, including scatter-gather tasks, etc.

### Running Dataflows

```bash
# Run all dataflows in a workspace (like `make`)
$ e3 start . production
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for changes and re-run affected dataflows
$ e3 start . production --watch
Watching inputs/... (Ctrl+C to stop)
```

`e3 start` topologically sorts dataflows by their input/output dependencies and executes them in order. Cached results are used when inputs haven't changed.
With the `--watch` flag it will use inotify to watch for changed values and propagate them.

### Dataflow DAG

The full DAG is implicit - all dataflows in the package, connected by their input/output paths:

```typescript
// These form a DAG: preprocess → train → predict
e3.dataflow("preprocess", {
    task: "preprocess",
    inputs: ["inputs/raw"],
    output: "cleaned",  // → outputs/preprocess/cleaned.east
});

e3.dataflow("train", {
    task: "train",
    inputs: ["outputs/preprocess/cleaned"],
    output: "model",  // → outputs/train/model.east
});

e3.dataflow("predict", {
    task: "predict",
    inputs: ["outputs/train/model", "inputs/new-data"],
    output: "forecast",  // → outputs/predict/forecast.east
});
```

### Selective Execution

```bash
# Run only dataflows matching a pattern
$ e3 start . production --filter "train*"

# Run a specific dataflow
$ e3 start . production train
```

## Related Documents

- **e3-core.md** - Core operations API
- **e3-cli.md** - Command-line interface reference
- **east.md** - East module system specification
