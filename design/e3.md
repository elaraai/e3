# e3: East Execution Engine

e3 is an execution engine for data projects. It manages reproducible computations across multiple runtimes (Node.js, Python, Julia), with content-addressed caching and reactive dataflow.

## Overview

e3 provides the infrastructure for "data science projects" - consulting engagements, ML pipelines, simulations, analytics - where you need to:

- **Iterate locally**: Write code, test with sample data, see results immediately
- **Deploy reliably**: Push the exact same computation to a server
- **Cache aggressively**: Never recompute what you've already computed
- **Mix runtimes**: Use scikit-learn from Python, simulations from Julia, glue logic in TypeScript
- **React to changes**: When input data updates, propagate changes through the pipeline

### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│  . (your e3 repository directory)                               │
│  ├── e3.east           # Project configuration                  │
│  ├── objects/          # Content-addressed storage              │
│  ├── packages/         # Installed packages (refs)              │
│  ├── tasks/            # Materialized tasks + execution state   │
│  └── workspaces/       # Stateful dataset namespaces            │
└─────────────────────────────────────────────────────────────────┘
```

The e3 repository contains and runs your workspaces, while also acting as a cache, execution log and package store.

Workspaces are where you work with data and follow the "template" defined in the package deployed to that workspace. You can type `e3 start <workspace>` to execute the dataflow to produce results.

Packages can serve different purposes, from support libraries to client project code. When a package is deployed to a workspace, so are all of its dependencies (meaning each workspace is associated with a given "root" or "main" package). Packages are immutable, and if you run the same package in a different workspace or e3 repository (even on a different computer), the same outputs will be reproduced.

e3 is agnostic to how code and packages are authored. You might create packages in TypeScript using the fluent API, extract them from another repository, or eventually code them in a native East syntax. East tasks execute compiled modules (IR), installed via packages.

### Key Concepts

**Workspaces** are namespaces of interactive datasets that you can read and write. Each workspace is initialized by deploying a package to the workspace, and contains the datasets and dataflows (tasks producing datasets).

**Packages** are an immutable bundle of e3 objects - East modules, tasks, datasets, dataflows, and install scripts for distribution. When you `e3 package add . east-python`, you get the Python runtime plus platform modules like `east-python/fs`. Packages can depend on other packages.

**Modules** are units of East code that export a value (a library typically exports a struct of functions, while a runnable program _is_ a module that exports its "main" function). Note that some modules are provided directly by East runtimes instead of being defined by East IR. See `east.md` for the module system specification.

**Runtimes** or "runners" are programs that can execute East programs as e3 tasks (e.g. our JavaScript interpretter or Julia compiler).

**Tasks** are a combination of a function module and the runtime that executes it, and defines programs that _can be_ run on e3.

**Executions** are tasks executions with specific inputs. A task's identity is the hash of its module plus its input hashes - same inputs always produce the same task ID, enabling memoization.

**Dataflows** are DAGs of tasks over files. Define which tasks read which files and produce which outputs, then `e3 start` executes the whole pipeline (like `make`).

### Example Workflow

```bash
# Initialize a repository for production data
$ cd ~/data/client-abc
$ e3 init .
Created e3 repository

# Install the Python runner from the public registry
$ e3 package add . east-python
Installing east-python... done

# Install your team's package from a local .zip
$ e3 package import . ~/dev/acme-forecast/dist/acme-forecast-1.2.0.zip
Installing acme-forecast@1.2.0... done

# Or add from a private registry
$ e3 registry add . https://packages.acme.internal
$ e3 package add . acme-forecast
Installing acme-forecast@0.21.1... done

# Run a task ad-hoc against data on your computer
$ e3 run . acme-forecast/train ./sales.beast2 -o ./model.beast2
Running acme-forecast/train... done (2.3s)

# Run again with same inputs - instant (cached)
$ e3 run . acme-forecast/train ./sales.beast2 -o ./model.beast2
Cached (0.01s)

# Create a workspace and deploy a package
$ e3 workspace create . production
Created production workspace

$ e3 workspace deploy . production acme-forecast@0.21.1
Deploying acme-forecast@0.21.1 to production... done

# Execute the full dataflow pipeline
$ e3 start . production
[1/3] preprocess... done (0.5s)
[2/3] train... done (38.1s)
[3/3] predict... done (1.2s)

# Get/set datasets
$ e3 dataset get . production outputs/predict
$ e3 dataset set . production inputs/sales ./new_sales.beast2

# Rerun - only affected dataflows execute
$ e3 start . production
[1/3] preprocess... cached
[2/3] train... cached
[3/3] predict... done (1.1s)

# Export workspace as a package (includes deployed package + current data)
$ e3 workspace export . production ./handoff.zip
Created acme-forecast-0.21.1-a3f8b2c1.zip

# Colleague imports it
$ e3 package import . ./handoff.zip
Installing acme-forecast@0.21.1-a3f8b2c1... done

$ e3 workspace deploy . analysis acme-forecast@0.21.1-a3f8b2c1
# Now they have your exact data state
```

## Repository Structure

An e3 repository is a directory containing configuration, content-addressed storage, and refs. Most files outside `objects/` are **refs** - small text files containing a SHA256 hash pointing to an object.

### Example Repository

```
~/data/client-abc/                    # Your e3 repository
├── e3.east                           # Config (registries, settings)
│
├── objects/                          # Content-addressed storage (all data lives here)
│   ├── 3a/
│   │   └── 8f2b...                   # A package object
│   ├── 7c/
│   │   └── 91d4...                   # A module IR blob
│   ├── a1/
│   │   └── bc56...                   # A dataset value
│   └── f2/
│       └── e847...                   # An execution result
│
├── packages/                         # Installed packages (refs)
│   ├── east-python/
│   │   └── 1.2.0                     # Contains: 3a8f2b... → objects/3a/8f2b...
│   └── acme-forecast/
│       ├── 0.20.0                    # Contains: 7d3e1a...
│       └── 0.21.1                    # Contains: 9b4c2f...
│
├── tasks/                            # Materialized tasks + execution state
│   └── 5e7a3b.../                    # Task hash (hash of task object)
│       ├── bin/                      # Runtime environment (venv, node_modules, etc.)
│       │   ├── run.sh                # Entry point generated by setup
│       │   └── venv/                 # Python example: uv-managed virtualenv
│       └── executions/               # Execution history for this task
│           └── c4f9a2.../            # Input hash (hash of all input values)
│               ├── stdout.txt        # Captured stdout (streamed during run)
│               ├── stderr.txt        # Captured stderr (streamed during run)
│               └── output            # Contains: f2e847... → objects/f2/e847...
│
└── workspaces/                       # Stateful dataset namespaces
    └── production/
        ├── package                   # Contains: acme-forecast/0.21.1
        └── datasets/
            ├── inputs/
            │   ├── sales             # Contains: a1bc56... → objects/a1/bc56...
            │   └── features          # Contains: 8d2e7f...
            └── outputs/
                ├── preprocess/
                │   └── cleaned       # Contains: b3c4d5...
                ├── train/
                │   └── model         # Contains: e6f7a8...
                └── predict/
                    └── forecast      # Contains: 1a2b3c...
```

### Directory Reference

#### `e3.east`

Repository configuration:

```east
(
    registries = {
        "default": "https://packages.east-lang.org",
        "acme": "https://packages.acme.internal",
    },
)
```

#### `objects/`

Content-addressed storage for all immutable data. Objects are stored by SHA256 hash, split into subdirectories by first two hex characters (like git). Everything ends up here: module IR, task definitions, dataset values, execution results, package manifests.

#### `packages/`

Installed packages. Each `<name>/<version>` file is a ref pointing to a package object in `objects/`. Package objects contain refs to their modules, tasks, dataflows, and datasets.

When you `e3 package add . acme-forecast`:
1. Package object and all referenced objects stored in `objects/`
2. Ref created at `packages/acme-forecast/0.21.1`

#### `tasks/`

Materialized task environments and execution history. Organized by task hash (the SHA256 of the task object, which includes the module and exec command).

**`tasks/<hash>/bin/`** - Runtime environment created on first execution:
- For Python tasks: `uv sync` creates a venv here
- For Node tasks: `npm install` creates node_modules here
- For Julia tasks: `julia --project=. instantiate` here
- `run.sh` is the generated entry point that invokes the task

**`tasks/<hash>/executions/<input_hash>/`** - State for each execution:
- `stdout.txt`, `stderr.txt` - Captured output (streamed live during execution)
- `output` - Ref to the result object

Execution identity = hash(task_hash, input_hashes...). Same inputs → same execution directory → cache hit.

#### `workspaces/`

Stateful namespaces where you work with data. Each workspace has:

- `package` - Ref to the deployed package (e.g., `acme-forecast/0.21.1`)
- `datasets/` - Mutable refs to dataset values

Workspaces follow the "template" defined by their package: the dataflows determine what tasks run and how datasets connect. But the actual data values can differ between workspaces (different inputs, different outputs).

### Refs Pattern

Most files outside `objects/` are refs - text files containing a SHA256 hash:

```bash
$ cat packages/acme-forecast/0.21.1
3a8f2b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a

$ cat workspaces/production/datasets/inputs/sales
a1bc56789def0123456789abcdef0123456789abcdef0123456789abcdef0123
```

This indirection enables:
- **Deduplication**: Same data referenced from multiple places
- **Atomic updates**: Write new object, then update ref
- **Easy diffing**: Compare ref values to detect changes

The exceptions are:
- `e3.east` - Configuration, not a ref
- `tasks/<hash>/bin/` - Materialized runtime environment
- `tasks/<hash>/executions/<hash>/*.txt` - Log files streamed during execution

### Object Types

All data in `objects/` is one of these types:

| Object | Description | Defined in |
|--------|-------------|------------|
| **Package** | Bundle of modules, tasks, dataflows, datasets | Packages section |
| **Module** | East IR + imports | east.md |
| **Task** | How to run a computation (init + run commands) | Tasks & Execution section |
| **Data** | East values (dataset contents, execution results) | — |

Objects reference each other by hash. A package contains hashes pointing to its modules and tasks. A task contains hashes pointing to module IR and init files.

### Garbage Collection

`e3 gc` removes unreferenced objects. The object graph is traced from roots:

**Roots:**
- `packages/<name>/<version>` → PackageObject hash
- `workspaces/<ws>/package` → `<name>/<version>` (resolved via packages/)
- `workspaces/<ws>/datasets/<path>` → Data object hash
- `tasks/<hash>/executions/<input_hash>/output` → Data object hash

**Object references:**

| Object | References |
|--------|------------|
| PackageObject | `modules` → ModuleObject hashes |
| | `tasks` → TaskObject hashes |
| | `datasets[].value` → Data object hashes |
| | `dependencies` → version strings (resolve via packages/) |
| TaskObject | `init_tree` → Data object hashes (package.json, etc.) |
| | `run[].object` → ModuleObject hashes (must include ALL modules) |
| ModuleObject | None (imports resolved at task level) |
| Data | None |

**Important:** The TaskObject's `run` command must include `--module` entries for the main module AND all its transitive imports. The SDK computes this closure when building the task.

### Bundled vs Installed

**Bundled** (`.zip` for distribution):
```
acme-forecast-0.21.1.zip
├── manifest.east     # Package name, version, root object hash
└── objects/
    ├── 3a/8f2b...
    ├── 7c/91d4...
    └── ...
```
- All objects inline in a zip file
- Self-contained, can be copied anywhere
- Produced by TypeScript build, `e3 package export`, or `e3 workspace export`
- Streaming I/O via `yauzl`/`yazl` (no need to load into RAM)

**Installed** (in repository):
- Objects extracted to `objects/`
- Ref created at `packages/<name>/<version>`
- Deduped with other packages
- Ready to deploy to workspaces

## Packages

A **package** bundles everything needed to run computations: modules, tasks, dataflows, and datasets. Packages are:
- **Defined in TypeScript** using the e3 SDK
- **Distributed as `.zip`** files (bundled form)
- **Installed as refs** to objects in the repository

### Package Object

A package object is stored in `objects/` and contains refs (hashes) to other objects:

```ts
type PackageObject = StructType<{
    name: StringType,
    version: StringType,

    // Refs to other objects (hash strings)
    modules: DictType<StringType, StringType>,    // name → module object hash
    tasks: DictType<StringType, StringType>,      // name → task object hash
    datasets: DictType<StringType, DatasetObject>,
    dataflows: DictType<StringType, DataflowObject>,

    // Package dependencies (resolved to specific versions)
    dependencies: DictType<StringType, StringType>,  // pkg name → version
}>;

type DatasetObject = StructType<{
    type: EastTypeValueType,          // East type of the data
    value: OptionType<StringType>,    // object hash of .beast2 file (if value exists)
}>;

type DataflowObject = StructType<{
    task: StringType,                 // task name in this package
    inputs: ArrayType<StringType>,    // input dataset names (paths)
    output: StringType,               // output dataset name
}>;
```

**Example package object:**

```
(
    name = "acme-forecast",
    version = "0.21.1",
    modules = {
        "train": "7c91d4...",           // → objects/7c/91d4...
        "predict": "a3f8b2...",
    },
    tasks = {
        "train": "5e7a3b...",           // → objects/5e/7a3b...
        "predict": "c4d5e6...",
    },
    dataflows = {
        "train": (task = "train", inputs = ["inputs/sales"], output = "model"),
        "predict": (task = "predict", inputs = ["outputs/train/model", "inputs/features"], output = "forecast"),
    },
    datasets = {
        "inputs/sales": (type = ArrayType(...), hash = .some "a1bc56..."),
        "inputs/features": (type = ArrayType(...), hash = .some "8d2e7f..."),
        "outputs/train/model": (type = StructType(...), hash = .none),
        "outputs/predict/forecast": (type = ArrayType(...), hash = .none),
    },
    dependencies = {
        "east-python": "1.2.0",
    },
)
```

### Module Object

Modules are stored as objects and passed to the runner via `--module` flags. Each module self-registers by name.

```ts
type ModuleObject = StructType<{
    name: StringType,                    // e.g. "acme-forecast/train"
    type: EastTypeValueType,             // exported type
    ir: IRType,                          // compiled East IR
    imports: SetType<StringType>,        // names of modules this imports
}>;
```

The module doesn't track where imports come from - that's resolved at the task level. The task object lists all `--module` paths, and the runner registers them by name before execution.

See **Tasks & Execution** for the task object structure, and **east.md** for the full module system specification.

### Bundled vs Installed

**Bundled** (`.zip` for distribution):
- All content inline in a zip file
- Self-contained, can be copied anywhere
- Produced by TypeScript build or `e3 workspace export`

**Installed** (in repository):
- Content stored in `objects/`
- Ref at `packages/<name>/<version>` points to package object
- Dependencies resolved to specific versions

```bash
# Import from local .zip
$ e3 package import . ~/dev/acme-forecast/dist/acme-forecast-0.21.0.zip

# Add from registry (fetches + imports)
$ e3 package add . acme-forecast@0.21.0

# Export an installed package to .zip
$ e3 package export . acme-forecast@0.21.0 ./acme-forecast-0.21.0.zip
```

### Package Namespacing

Package names provide namespaces for their contents:

```bash
# Run a task from a package
$ e3 run . acme-forecast/train inputs/sales.east -o model.beast2

# Reference a module from another package (in East code)
const ml = $.import("east-python/ml");
```

### Creating Packages

Packages are defined in TypeScript using the e3 SDK:

```typescript
import { East } from '@elaraai/east';
import { e3 } from '@elaraai/e3-sdk';
import mlPkg from '@elaraai/python-ml';

// Write the code as an East module
const trainModule = East.module("train", ($) => {
    const ml = $.import(mlPkg.modules.ml);

    // ... East code
});

// input dataset - loaded from ./inputs/sales.* (user can provide .beast2, .east or .json)
const sales = e3.input("sales", ArrayType(...), /* default value goes here */)

// Define a dataflow step from a module
const pipeline = e3.dataflow("train", [sales], trainModule); 

// Or define it inline
const pipeline = e3.dataflow("train", [sales], $ => { ... }); 

// Construct the package bundle
const pkg = e3.package(
    {
        name: "acme-forecast",
        version: "0.21.0",
    },
    pipeline, // automatically infers any dependencies (input datasets, upstream dataflows, modules, package dependencies, etc)
    // ... can add more
);

await pkg.save(/* defaults to save at ./acme-forecast-0.21.0.zip */);

export default pkg; // ready to import by other packages
```

There is a lot of freedom to import and bundle "pure" East modules defined in other npm packages, dynamically link with e3 packages, define logic inline, or more.

Simply run the script to produce the `acme-forecast-0.21.0.zip` bundle.

## Tasks & Execution

A **task** is an object that defines how to run a computation. Tasks are stored in `objects/` and referenced by packages.

### Task Object

```ts
type TaskObject = StructType<{
    // Files to materialize in bin/ before init runs
    init_tree: DictType<StringType, StringType>,  // path → object hash

    // Run once after tree is materialized (optional)
    init: OptionType<ArrayType<CommandPartType>>,

    // Run for each execution
    run: ArrayType<CommandPartType>,
}>;

type CommandPart = VariantType<{
    literal: StringType, // Literal string: "npm", "run", "--prefix"
    object: StringType,  // Path to object: ./objects/<hash>
    bin: NullType,       // Path to this task's bin/ directory
    input: IntegerType,  // Path to input N
    output: NullType,    // Path where output should be written
}>;
```

### Example: Node.js Task

```east
(
    init_tree = {
        "package.json": "abc123...",
        "package-lock.json": "def456...",
    },
    init = .some [
        .literal "npm",
        .literal "install",
        .literal "--prefix",
        .bin,
    ],
    run = [
        .bin, .literal "/node_modules/.bin/east",
        .literal "run",
        .literal "--runtime", .literal "@elaraai/east-node-fs",
        .literal "--module", .object "7c91d4...",
        .literal "--main", .literal "acme-forecast/train",
        .literal "--input", .input 0,
        .literal "--output", .output,
    ],
)
```

The `package.json` object contains:

```json
{
    "dependencies": {
        "@elaraai/East": "1.0.0",
        "@elaraai/east-node-fs": "1.2.0"
    }
}
```

### The East CLI

The `east` CLI (from `@elaraai/East`) runs East modules:

```bash
east run \
  --runtime @elaraai/east-node-fs \
  --module ./objects/7c91d4... \
  --main acme-forecast/train \
  --input ./inputs/sales.beast2 \
  --output ./outputs/model.beast2
```

- `--runtime <pkg>` - npm packages providing runtime modules (self-register their module names)
- `--module <path>` - paths to East IR module objects (self-register their names)
- `--main <name>` - which module to execute
- `--input <path>` - input data files
- `--output <path>` - where to write result

Runtime module packages export a record keyed by module name:

```typescript
// @elaraai/east-node-fs
export default {
    "east-node/fs": { 
        readFile: (path: EastString) => /* ... */,
        writeFile: (path: EastString, data: EastBytes) => /* ... */,
    },
};
```

### Execution

An **execution** is a task invoked with specific inputs. Execution identity:

```
input_hash = hash(input_hashes...)
```

Same task + same inputs = same execution directory = cache hit.

### Running Tasks

```bash
# Ad-hoc run (specify I/O explicitly)
$ e3 run . acme-forecast/train inputs/sales.east -o outputs/model.beast2
Running acme-forecast/train... done (2.3s)

# Run again - cache hit
$ e3 run . acme-forecast/train inputs/sales.east -o outputs/model.beast2
Cached (0.01s)

# Run from a different package
$ e3 run . other-pkg/preprocess inputs/raw.json -o inputs/clean.east
```

### Execution Process

Each task execution runs as a **separate process**:

1. e3 looks up the task object from `objects/`
2. If `tasks/<hash>/bin/` doesn't exist:
   - Materialize `init_tree` files into `bin/`
   - Run the `init` command (if present)
3. Run the `run` command, substituting:
   - `.bin` → `tasks/<hash>/bin/`
   - `.object <hash>` → `objects/<hash>`
   - `.input N` → path to input N
   - `.output` → output path
4. Store result in `objects/`, write ref to `tasks/<hash>/executions/<input_hash>/output`
5. Stdout/stderr streamed to `tasks/<hash>/executions/<input_hash>/*.txt`

### Memoization

Executions are cached by their ID (hash of task + inputs). When you run a task:

1. Compute input hash from input content hashes
2. Check if `tasks/<task_hash>/executions/<input_hash>/output` exists
3. If cached: return result immediately (read ref, fetch from objects/)
4. If not: execute, store result, write ref

This means:
- Changing inputs → new execution (cache miss)
- Changing module code → new task hash → new execution
- Re-running unchanged computation → instant cache hit

## Dataflows

A **dataflow** connects a task to dataset paths. Dataflows are defined inline in the package object (not separate objects).

```east
dataflows = {
    "train": (task = "train", inputs = ["inputs/sales"], output = "model"),
    "predict": (task = "predict", inputs = ["outputs/train/model"], output = "forecast"),
}
```

- `task` - name of a task in this package
- `inputs` - dataset paths to read
- `output` - name of output dataset (stored at `outputs/<dataflow>/output`)

In TypeScript:

```typescript
e3.dataflow("train", [sales], trainTask);
```

In future, we'll add the ability to destructure outputs to allow multiple named outputs.
We can similarly generalize inputs to enable different patterns, including scatter-gather tasks, etc.

### Running Dataflows

```bash
# Run all dataflows in a workspace (like `make`)
$ e3 start . production
[1/3] preprocess... done (0.5s)
[2/3] train... cached
[3/3] predict... done (1.2s)

# Watch for changes and re-run affected dataflows
$ e3 start . production --watch
Watching inputs/... (Ctrl+C to stop)
```

`e3 start` topologically sorts dataflows by their input/output dependencies and executes them in order. Cached results are used when inputs haven't changed.
With the `--watch` flag it will use inotify to watch for changed values and propagate them.

### Dataflow DAG

The full DAG is implicit - all dataflows in the package, connected by their input/output paths:

```typescript
// These form a DAG: preprocess → train → predict
e3.dataflow("preprocess", {
    task: "preprocess",
    inputs: ["inputs/raw"],
    output: "cleaned",  // → outputs/preprocess/cleaned.east
});

e3.dataflow("train", {
    task: "train",
    inputs: ["outputs/preprocess/cleaned"],
    output: "model",  // → outputs/train/model.east
});

e3.dataflow("predict", {
    task: "predict",
    inputs: ["outputs/train/model", "inputs/new-data"],
    output: "forecast",  // → outputs/predict/forecast.east
});
```

### Selective Execution

```bash
# Run only dataflows matching a pattern
$ e3 start . production --filter "train*"

# Run a specific dataflow
$ e3 start . production train
```

## CLI Reference

All commands take a repository path as the first argument (`.` for current directory).

### Repository

```bash
e3 init <repo>                              # Create new repository
e3 status <repo>                            # Show installed packages, workspaces
e3 gc <repo>                                # Remove unreferenced objects
```

### Packages

```bash
e3 package add <repo> <name>[@<ver>]        # Fetch from registry + import
e3 package import <repo> <path.zip>         # Import from local .zip
e3 package export <repo> <pkg>[@<ver>] <path.zip>  # Export to .zip
e3 package remove <repo> <pkg>[@<ver>]      # Remove package
e3 package list <repo>                      # List installed packages
```

### Workspaces

```bash
e3 workspace create <repo> <name>           # Create empty workspace
e3 workspace deploy <repo> <ws> <pkg>[@<ver>]  # Deploy package to workspace
e3 workspace export <repo> <ws> <path.zip>  # Export workspace as package
    [--name <pkg>] [--version <ver>]        # Default: <pkg>@<ver>-<hash>
e3 workspace list <repo>                    # List workspaces
e3 workspace remove <repo> <ws>             # Remove workspace
```

### Datasets

```bash
e3 dataset get <repo> <ws> <path>           # Print dataset value
e3 dataset set <repo> <ws> <path> <file>    # Set dataset from file
e3 dataset list <repo> <ws>                 # List datasets in workspace
```

### Execution

```bash
e3 run <repo> <task> <inputs...> -o <out>   # Ad-hoc task execution
e3 start <repo> <ws> [--filter <pattern>]   # Run dataflows in workspace
e3 start <repo> <ws> --watch                # Watch mode - re-run on changes
```

### Inspection

```bash
e3 logs <repo> [<task_hash>] [--follow]     # View execution logs
e3 view <repo> <path>                       # TUI data viewer
e3 convert <path> [--format east|json]      # Convert between formats
```

### Registry

```bash
e3 registry add <repo> <url>                # Add registry
e3 registry remove <repo> <url>             # Remove registry
e3 registry list <repo>                     # List registries
e3 publish <repo> <pkg>[@<ver>]             # Publish to registry
e3 search <repo> <query>                    # Search registries
```

### Examples

```bash
# Setup
$ e3 init .
$ e3 package add . east-python
$ e3 package import . ~/dev/my-pkg/dist/my-pkg-1.0.0.zip

# Create workspace and run
$ e3 workspace create . production
$ e3 workspace deploy . production my-pkg@1.0.0
$ e3 start . production
$ e3 dataset get . production outputs/predict

# Update data and rerun
$ e3 dataset set . production inputs/sales ./new_sales.beast2
$ e3 start . production

# Export workspace state for colleague
$ e3 workspace export . production ./handoff.zip
# Creates my-pkg-1.0.0-a3f8b2c1.zip (includes package + current data)

# Colleague imports and works with your exact state
$ e3 package import . ./handoff.zip
$ e3 workspace deploy . analysis my-pkg@1.0.0-a3f8b2c1
```
